{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![hslu_logo.png](img/hslu_logo.png)\n",
    "\n",
    "## Week 01\n",
    "\n",
    "<hr style=\"border:1px solid black\">\n",
    "\n",
    "# Exercise: Optimization using PyTorch - using autograd option\n",
    "---\n",
    "This exercise is to illustrate the optimization procedure using the fit of a sine-function with a polynomial of a given degree.  We use the possibility of pytorch to determine gradients of tensors automatically, the so-called autograd option.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a function and plot it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = torch.linspace(-math.pi, math.pi, 50, dtype=torch.float)\n",
    "data_y = torch.sin(data_x)\n",
    "\n",
    "\n",
    "sin_fct = plt.plot(data_x,data_y,'b.-',label='sin(x)')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.axis([-math.pi, math.pi,-1,1])\n",
    "plt.xticks((-math.pi,-math.pi/2,0,math.pi/2,math.pi),('$-\\pi$','$-\\pi$/2','0','$\\pi$/2','$\\pi$'))\n",
    "plt.legend()\n",
    "plt.show(sin_fct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define class for fitting the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class poly_fit():\n",
    "    \"\"\"\n",
    "    fit function  \n",
    "    \"\"\"\n",
    "    def __init__(self, degree):\n",
    "        \"\"\"\n",
    "        constructor\n",
    "\n",
    "        Arguments:\n",
    "        degree -- of polynomial (# of fit parameters)\n",
    "        \n",
    "        \"\"\"\n",
    "        self.degree = degree\n",
    "        # initialize weights and bias (zero or random)\n",
    "        self.initialise_weights()\n",
    "    \n",
    "    \n",
    "    def initialise_weights(self):\n",
    "        \"\"\"\n",
    "        initialise weights\n",
    "        \"\"\"\n",
    "\n",
    "        ### START YOUR CODE ###\n",
    "\n",
    "        #extend the initialization with autograd functionality\n",
    "\n",
    "        # initialize weights (default is row vector) and bias \n",
    "        self.W = torch.randn([self.degree])      \n",
    "        self.B = torch.randn(1)\n",
    "        \n",
    "        ### END YOUR CODE ###      \n",
    "        \n",
    "    def propagate(self, X):\n",
    "        \"\"\"\n",
    "        calculates the function estimation based on current parameters [W,B]\n",
    "        \"\"\"    \n",
    "        self.Y_pred = self.W @ X + self.B\n",
    "\n",
    "        return self.Y_pred\n",
    "           \n",
    "     \n",
    "    def back_propagate(self, cost):\n",
    "        \"\"\"\n",
    "        calculates the backpropagation results based on expected output y\n",
    "        this function must be performed AFTER the corresponding propagate step\n",
    "        \"\"\"    \n",
    "        cost.backward()\n",
    " \n",
    "\n",
    "    def calc_cost(self, Y):\n",
    "        \"\"\"\n",
    "        calculates the MSE loss function\n",
    "        \"\"\"\n",
    "        cost = 1/2*(self.Y_pred - Y).pow(2).mean()\n",
    "        \n",
    "        return cost\n",
    "    \n",
    "        \n",
    "        \n",
    "    def gradient_descend(self, alpha):\n",
    "        \"\"\"\n",
    "        does the gradient descend based on results from last back_prop step with learning rate alpha\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            self.W -= alpha * self.W.grad\n",
    "            self.B -= alpha * self.B.grad\n",
    "\n",
    "        ### START YOUR CODE ###\n",
    "        #clear the gradient members of W and B\n",
    "    \n",
    "        #self.W.grad = \n",
    "        #self.B.grad = \n",
    "    \n",
    "        ### END YOUR CODE ###\n",
    "        \n",
    "        \n",
    "    def optimize(self, data, epochs, alpha, debug=0):\n",
    "        \"\"\"\n",
    "        performs epochs number of gradient descend steps and appends result to output array\n",
    "\n",
    "        Arguments:\n",
    "        data -- dictionary with data\n",
    "        epochs -- number of epochs\n",
    "        alpha -- learning rate\n",
    "        debug -- False (default)/True; get info on each gradient descend step\n",
    "        \"\"\"\n",
    "        \n",
    "        # save results before 1st step\n",
    "        for i0 in range(0, epochs):\n",
    "            #do prediction\n",
    "            self.propagate(data['X_train'])\n",
    "            #determine the loss \n",
    "            cost = self.calc_cost(data['Y_train'])\n",
    "            #determine the error\n",
    "            self.back_propagate(cost)\n",
    "            #do the correction step\n",
    "            self.gradient_descend(alpha)\n",
    "            \n",
    "            if debug and np.mod(i0, debug) == 0:\n",
    "                print('step %r, cost %r' % (i0, cost))\n",
    "                        \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define X and Y values and do optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fix the degree\n",
    "degree = 3\n",
    "\n",
    "data_X = torch.zeros([degree, data_x.shape[0]])\n",
    "for i0 in range(0, degree):\n",
    "    data_X[i0,:] = (data_x/math.pi).pow(1+2*i0)\n",
    "\n",
    "data = {'X_train' : data_X, 'Y_train' : data_y}\n",
    "\n",
    "poly_fit_inst = poly_fit(degree)\n",
    "\n",
    "poly_fit_inst.optimize(data, 40000, 0.1,10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    y_pred = poly_fit_inst.propagate(data_X).numpy()\n",
    "\n",
    "sin_fct = plt.plot(data_x,data_y,'b.', label='sin(x)')\n",
    "plt.plot(data_x,y_pred,'r-', label='fit')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.axis([-math.pi, math.pi,-1,1])\n",
    "plt.xticks((-math.pi,-math.pi/2,0,math.pi/2,math.pi),('$-\\pi$','$-\\pi$/2','0','$\\pi$/2','$\\pi$'))\n",
    "plt.legend()\n",
    "plt.show(sin_fct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
