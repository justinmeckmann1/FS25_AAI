{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![hslu_logo.png](img/hslu_logo.png)\n",
    "\n",
    "## Week 01\n",
    "\n",
    "<hr style=\"border:1px solid black\">\n",
    "\n",
    "# Exercise: Introduction to PyTorch\n",
    "---\n",
    "---\n",
    "\n",
    "*PyTorch is a machine learning framework based on the Torch library,used for applications such as computer vision and natural language processing,originally developed by Meta AI and now part of the Linux Foundation umbrella. It is recognized as one of the two most popular machine learning libraries alongside TensorFlow, offering free and open-source software released under the modified BSD license. Although the Python interface is more polished and the primary focus of development, PyTorch also has a C++ interface.* [Wikipedia](https://en.wikipedia.org/wiki/PyTorch).\n",
    "\n",
    "Due to its similarity with numpy - with regard to array (or tensor) management - you can quickly familiarize with PyTorch if you have a good knowledge of numpy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import PyTorch and numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling of tensors\n",
    "#### Convert from numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]]\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "array_np = np.ones([3,2])\n",
    "print(array_np)\n",
    "\n",
    "array_py = torch.from_numpy(array_np)\n",
    "print(array_py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.]])\n",
      "tensor([[0.4880, 0.3375, 0.1098],\n",
      "        [0.4233, 0.0219, 0.6287]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones([3,2],dtype=torch.float)\n",
    "print(x)\n",
    "\n",
    "y = torch.rand([2,3],dtype=torch.float)\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Operation on tensors \n",
    "\n",
    "Observe the following multiplications. The multiplication sign `*` represents an element-wise multiplication (including possible broadcasting as in numpy) and `@` is a multiplication in the mathematical sense i.e., matrix multiplication.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4880, 0.4233],\n",
      "        [0.3375, 0.0219],\n",
      "        [0.1098, 0.6287]])\n",
      "tensor([[0.9113, 0.3594, 0.7385],\n",
      "        [0.9113, 0.3594, 0.7385],\n",
      "        [0.9113, 0.3594, 0.7385]])\n",
      "tensor([[0.9353, 0.9353],\n",
      "        [1.0739, 1.0739]])\n",
      "tensor([[0.9113, 0.9113, 0.9113],\n",
      "        [0.3594, 0.3594, 0.3594],\n",
      "        [0.7385, 0.7385, 0.7385]])\n"
     ]
    }
   ],
   "source": [
    "print(x*y.T)  # Transponierte --> Komponentenweise Multiplikation\n",
    "print(x@y)\n",
    "print(y@x)\n",
    "print(y.T@x.T)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indexing\n",
    "\n",
    "Indexing of torch tensors is as in numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4880, 0.4233])\n",
      "tensor([0.4233, 0.0219, 0.6287])\n",
      "tensor([[False, False, False],\n",
      "        [False, False,  True]])\n"
     ]
    }
   ],
   "source": [
    "print(y[:,0])\n",
    "print(y[1,:])\n",
    "print(y > 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assignemnt / Copy\n",
    "\n",
    "Care has to be taken - as in numpy - when using an assignment of a `x` to a tensor `y`. This does $not$ represent a deep copy and changes to the elements of `y` will also change `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.]])\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.]])\n",
      "tensor([[2., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.]])\n",
      "tensor([[2., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones([3,2],dtype=torch.float)\n",
    "y = torch.rand([2,3],dtype=torch.float)\n",
    "y = x # call by reference\n",
    "print(x)\n",
    "print(y)\n",
    "y[0,0] = 2 # ändert auch x\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a deep copy with the `copy_` operator to ensure a new tensor with own elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.]])\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.]])\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.]])\n",
      "tensor([[2., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones([3,2],dtype=torch.float)\n",
    "y = torch.rand([3,2],dtype=torch.float)\n",
    "y.copy_(x)\n",
    "print(x)\n",
    "print(y)\n",
    "y[0,0] = 2\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autograd functionality\n",
    "PyTorch provides the possibility to automatically keep track of the gradients with respect to certain tensors. In the following example, PyTorch will keep track of the gradients with respect to `W` and `B`when calculating the result `res`. This is done by calling `res.backward()`. The gradients are then available in the `grad` member of the respective tensor.\n",
    "\n",
    "#### Automatic gradient using autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.],\n",
      "        [2.],\n",
      "        [3.]], dtype=torch.float64)\n",
      "tensor([[ 0.4002,  0.5487,  0.9007],\n",
      "        [-0.1654,  0.1927, -1.3228]], dtype=torch.float64, requires_grad=True)\n",
      "tensor([[-0.5270],\n",
      "        [-0.2213]], dtype=torch.float64, requires_grad=True)\n",
      "a= tensor([[ 3.6726],\n",
      "        [-3.9696]], dtype=torch.float64, grad_fn=<AddBackward0>) torch.Size([2, 1])\n",
      "cost= 29.245965535544048\n",
      "[[  7.34516629  14.69033258  22.03549887]\n",
      " [ -7.93929432 -15.87858864 -23.81788297]]\n",
      "[[ 7.34516629]\n",
      " [-7.93929432]]\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1,2,3]],dtype=torch.double).T\n",
    "#declear W and b as tensors with gradient determination\n",
    "W = torch.randn([2,3],dtype=torch.double, requires_grad=True) # requires_grad=True --> we want to calculate the gradient of W\n",
    "b = torch.randn([2,1],dtype=torch.double, requires_grad=True) \n",
    "\n",
    "print(x)\n",
    "print(W)\n",
    "print(b)\n",
    "\n",
    "#calculate a function called 'cost'\n",
    "a = W@x + b         # matrix multiplication \n",
    "print('a=',a, a.shape)\n",
    "cost = a.T@a        # quadratstumme \n",
    "print('cost=',cost.item())\n",
    "\n",
    "#now call backward() on 'cost' to determine the gradients of W and b\n",
    "cost.backward()     # berechnet für alle verwendeten variablen die partiellen ableitungen, diese stehen dann in W.grad und b.grad zur verfügung\n",
    "\n",
    "#print the result\n",
    "print(W.grad.numpy())\n",
    "print(b.grad.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to notice, that the `grad` member is not cleared automatically before a call to `backward()`. Thus successive calls will accumulate the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.245965535544048\n",
      "[[ 22.03549887  44.07099773  66.1064966 ]\n",
      " [-23.81788297 -47.63576593 -71.4536489 ]]\n",
      "[[ 22.03549887]\n",
      " [-23.81788297]]\n"
     ]
    }
   ],
   "source": [
    "#repeat the same calculation \n",
    "a = W@x + b\n",
    "cost = a.T@a\n",
    "print(cost.item())\n",
    "\n",
    "#now call backward() on 'cost' to determine the gradients of W and b\n",
    "cost.backward()\n",
    "\n",
    "#print the result\n",
    "print(W.grad.numpy())\n",
    "print(b.grad.numpy())\n",
    "\n",
    "#it will differ and increase with each call --> Das Resultat wird bei jedem Aufruf von backward() addiert --> Deswegen sollte W.grad = None vor jedem Aufruf gesetzt werden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, the `grad` member of `W` and `b` must be reset before each call to `backward()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.245965535544048\n",
      "[[  7.34516629  14.69033258  22.03549887]\n",
      " [ -7.93929432 -15.87858864 -23.81788297]]\n",
      "[[ 7.34516629]\n",
      " [-7.93929432]]\n"
     ]
    }
   ],
   "source": [
    "#repeat the same calculation \n",
    "a = W@x + b\n",
    "cost = a.T@a\n",
    "print(cost.item())\n",
    "\n",
    "#clear the grad entry of W and b\n",
    "W.grad = None\n",
    "b.grad = None\n",
    "\n",
    "#now call backward() on 'cost' to determine the gradients of W and b\n",
    "cost.backward()\n",
    "\n",
    "#print the result\n",
    "print(W.grad.numpy())\n",
    "print(b.grad.numpy())\n",
    "\n",
    "#now the results will be again identical as in the first call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A further problem may arise, once you want to use the values of `W` and `b` for calculation. Variables that required gradient calculations are restricted in their use. But you can always call `with torch.no_grad()` in order to suppress the gradient calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the following call (W += ...) will raise an error as long as the first statement (with torch.no_grad():) is commented\n",
    "#\n",
    "with torch.no_grad():\n",
    "    W += W.grad\n",
    "    W -= W.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manual verification of gradient determination\n",
    "\n",
    "For comparison we determine the gradient manually. Recall that \n",
    "$$\n",
    "res = \\mathbf{a}^T \\cdot \\mathbf{a} = (\\mathbf{W} \\cdot \\mathbf{x} + \\mathbf{b})^T \\cdot (\\mathbf{W} \\cdot \\mathbf{x} + \\mathbf{b}) \n",
    "$$\n",
    "Thus it is straight forward to show that:\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\mathbf{W}} res = 2 \\cdot (\\mathbf{W} \\cdot \\mathbf{x} + \\mathbf{b}) \\cdot \\mathbf{x}^T = 2\\cdot \\mathbf{a} \\cdot \\mathbf{x}^T\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\mathbf{b}} res = 2 \\cdot (\\mathbf{W} \\cdot \\mathbf{x} + \\mathbf{b})= 2\\cdot \\mathbf{a}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.41061076 -4.82122152 -7.23183229]\n",
      " [-1.4430012  -2.8860024  -4.3290036 ]]\n",
      "[[-2.41061076]\n",
      " [-1.4430012 ]]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    W_grad = 2*a*x.T # Transponierte --> Komponentenweise Multiplikation\n",
    "    b_grad = 2*a\n",
    "    print(W_grad.numpy())\n",
    "    print(b_grad.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numeric gradient determination\n",
    "\n",
    "Finally, again for comparison we determine the gradient numerically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.4106, -4.8212, -7.2318],\n",
      "        [-1.4430, -2.8860, -4.3290]], dtype=torch.float64,\n",
      "       grad_fn=<CopySlices>)\n",
      "tensor([[-2.4106],\n",
      "        [-1.4430]], dtype=torch.float64, grad_fn=<CopySlices>)\n"
     ]
    }
   ],
   "source": [
    "eps = 1e-7\n",
    "W_grad = torch.zeros(W.shape,dtype=torch.double)\n",
    "for row in range(0, W.shape[0]):\n",
    "    for col in range(0, W.shape[1]):\n",
    "        dw = torch.zeros(W.shape,dtype=torch.double)\n",
    "        dw[row,col] = eps\n",
    "        a_eps = (W+dw)@x + b\n",
    "        cost_eps = a_eps.T@a_eps\n",
    "        W_grad[row,col] = (cost_eps - cost)/eps\n",
    "        \n",
    "print(W_grad)\n",
    "\n",
    "b_grad = torch.zeros(b.shape,dtype=torch.double)\n",
    "for row in range(0, b.shape[0]):\n",
    "    db = torch.zeros(b.shape,dtype=torch.double)\n",
    "    db[row,0] = eps\n",
    "    a_eps = W@x + b + db\n",
    "    cost_eps = a_eps.T@a_eps\n",
    "    b_grad[row,0] = (cost_eps - cost)/eps\n",
    "\n",
    "print(b_grad)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "FS25_AAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
