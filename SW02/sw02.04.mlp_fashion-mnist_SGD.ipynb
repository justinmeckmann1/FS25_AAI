{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![hslu_logo.png](img/hslu_logo.png)\n",
    "\n",
    "## Week 02\n",
    "\n",
    "<hr style=\"border:1px solid black\">\n",
    "\n",
    "# Excercise: Multi Layer Perceptron with PyTorch\n",
    "---\n",
    "---\n",
    "This excercise is to illustrate a first classification problem using a multi-layer-perceptron. In other words it is a feed forward network with a set of fully connected layers. The input data can be choosen to be MNIST or FashionMNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils import read_data, plot_img, plot_tiles, plot_error, plot_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download dataset (MNIST or FashionMNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data is save in storage_path (use same path for different SW)\n",
    "training_data, test_data, labels_map = read_data(data_type='FashionMNIST', storage_path='../SW01/data')\n",
    "\n",
    "#to access the images\n",
    "print(f'training data shape {training_data.data.shape} and type {training_data.data.dtype} ')\n",
    "print(f'test data shape {test_data.data.shape} and type {test_data.data.dtype} ')\n",
    "\n",
    "#to access the labels\n",
    "print(f'training data shape {training_data.targets.shape} and type {training_data.targets.dtype} ')\n",
    "print(f'test data shape {test_data.targets.shape} and type {test_data.targets.dtype} ')\n",
    "print(f'categories: {labels_map}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Organization of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to access direclty the images\n",
    "print('data shape and type:')\n",
    "print(training_data.data.shape)\n",
    "print(training_data.data.dtype)\n",
    "\n",
    "#to access directly the lables\n",
    "print('\\ncategory labels with shape and type:')\n",
    "print(torch.unique(training_data.targets))\n",
    "print(training_data.targets.shape)\n",
    "print(training_data.targets.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot sample images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_img(training_data.data[0], figure_size = [2,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot categories mixed\n",
    "plot_tiles(training_data.data, 10, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot only certain categorie\n",
    "label = 3\n",
    "plot_tiles(training_data.data[training_data.targets == label], 10,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MiniBatch class\n",
    "Pytorch has its own dataloader routine for mini batches but the present version is more efficient for our toy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniBatches:\n",
    "    \"\"\"\n",
    "    obtains x- and y-data in the constructor and returns a sample of batch_size with each call to next()\n",
    "    \"\"\"\n",
    "    def __init__(self, X, Y, batch_size):\n",
    "        \"\"\"\n",
    "        constructor\n",
    "\n",
    "        Arguments:\n",
    "        x/y -- data\n",
    "        batch_size -- size of batch (0 means one single batch)\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        m = X.shape[0]\n",
    "        self.indices = torch.randperm(m)\n",
    "        self.n = X.shape[1]\n",
    "        \n",
    "        if not batch_size:\n",
    "            self.batch_size = m\n",
    "            self.mb = 1\n",
    "        else:\n",
    "            self.batch_size = batch_size        \n",
    "            self.mb = int(m / self.batch_size)    \n",
    "        \n",
    "        self.ib = 0\n",
    "\n",
    "    def number_of_batches(self):\n",
    "        return self.mb\n",
    "\n",
    "    def next(self):\n",
    "        it = self.indices[self.ib * self.batch_size:(self.ib + 1) * self.batch_size]\n",
    "        X_batch = self.X[it, :]\n",
    "        Y_batch = self.Y[it]\n",
    "        self.ib += 1\n",
    "\n",
    "        return {'X_batch': X_batch, 'Y_batch': Y_batch}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron:\n",
    "    \"\"\"\n",
    "    MLP class handling the layers and doing all propagation and back propagation steps\n",
    "    all hidden layers are dense (with ReLU activation) and the last layer is softmax\n",
    "    \"\"\"\n",
    "    def __init__(self, num_input, num_hidden, num_output):\n",
    "        \"\"\"\n",
    "        constructor\n",
    "\n",
    "        Arguments:\n",
    "        list_num_neurons -- list of layer sizes including in- and output layer\n",
    "        \n",
    "        \"\"\"\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(num_input, num_output)\n",
    "            ### START YOUR CODE ###*\n",
    "            #torch.nn....,\n",
    "            #torch.nn...\n",
    "            ### END YOUR CODE ###*\n",
    "        )\n",
    "        \n",
    "        self.cost_fn = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "        #used to save results\n",
    "        self.result_data = torch.tensor([])\n",
    "        \n",
    "    def propagate(self, X):\n",
    "        \"\"\"\n",
    "        calculates the function estimation based on current parameters [W,B]\n",
    "        \"\"\"    \n",
    "        self.Y_pred = self.model(X)\n",
    "           \n",
    "     \n",
    "    def back_propagate(self, cost):\n",
    "        \"\"\"\n",
    "        calculates the backpropagation results based on expected output y\n",
    "        this function must be performed AFTER the corresponding propagte step\n",
    "        \"\"\"    \n",
    "        self.model.zero_grad()\n",
    "        \n",
    "        cost.backward()\n",
    " \n",
    "\n",
    "    def calc_cost(self, Y):\n",
    "        \"\"\"\n",
    "        calculates the MSE loss function\n",
    "        \"\"\"\n",
    "        cost = self.cost_fn(self.Y_pred, Y)\n",
    "        \n",
    "        return cost\n",
    "    \n",
    "        \n",
    "        \n",
    "    def gradient_descend(self, alpha):\n",
    "        \"\"\"\n",
    "        does the gradient descend based on results from last back_prop step with learning rate alpha\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            for param in self.model.parameters():\n",
    "                param -= alpha * param.grad\n",
    "            \n",
    "     \n",
    "        \n",
    "    def calc_error(self, Y):\n",
    "        \"\"\"\n",
    "        get error information\n",
    "        \"\"\"\n",
    "        m = Y.shape[0]\n",
    "\n",
    "        Y_pred_argmax = torch.argmax(self.Y_pred, dim=1)\n",
    "        train_error = torch.sum(Y != Y_pred_argmax) / m\n",
    "\n",
    "        return train_error\n",
    "\n",
    "\n",
    "    def save_training_data(self, data):\n",
    "        \"\"\"\n",
    "        save training and validation curves\n",
    "        \"\"\"\n",
    "        #determine the train loss and error\n",
    "        self.propagate(data['X_train'])\n",
    "        cost_train = self.calc_cost(data['Y_train'])\n",
    "        error_train = self.calc_error(data['Y_train'])\n",
    "        #calculate validation loss and error\n",
    "        self.propagate(data['X_val'])\n",
    "        cost_val = self.calc_cost(data['Y_val'])\n",
    "        error_val = self.calc_error(data['Y_val'])\n",
    "\n",
    "        #safe the results\n",
    "        res = torch.tensor([[cost_train.item(), error_train.item(), cost_val.item(), error_val.item()]])\n",
    "        self.result_data = torch.cat((self.result_data, res), 0)\n",
    "        \n",
    "        \n",
    "    def optimize(self, data, epochs, alpha, batch_size=16, debug=0):\n",
    "        \"\"\"\n",
    "        performs epochs number of gradient descend steps and appends result to output array\n",
    "\n",
    "        Arguments:\n",
    "        data -- dictionary with data\n",
    "        epochs -- number of epochs\n",
    "        alpha -- learning rate\n",
    "        batch_size -- size of batch (0: use full training set)\n",
    "        debug -- False (default)/True; get info on each gradient descend step\n",
    "        \"\"\"\n",
    "        \n",
    "        # save results before 1st step\n",
    "        for i0 in range(0, epochs):\n",
    "            #save the data at the beginning of the step\n",
    "            self.save_training_data(data)\n",
    "            #create batches for each epoch\n",
    "            batches = MiniBatches(data['X_train'], data['Y_train'], batch_size)\n",
    "            #loop over batches\n",
    "            for ib in range(batches.number_of_batches()):\n",
    "                batch = batches.next()\n",
    "                #do prediction\n",
    "                self.propagate(batch['X_batch'])\n",
    "                #determine the loss \n",
    "                cost = self.calc_cost(batch['Y_batch'])\n",
    "                #determine the error\n",
    "                self.back_propagate(cost)\n",
    "                #do the correction step\n",
    "                self.gradient_descend(alpha)\n",
    "                #calculate the error\n",
    "                error = self.calc_error(batch['Y_batch'])\n",
    "    \n",
    "            if debug and np.mod(i0, debug) == 0:\n",
    "                print('step %r, train cost %1.3f, train error %1.3f, val cost %1.3f, val error %1.3f' % \\\n",
    "                     (i0, self.result_data[-1,0], self.result_data[-1,1], self.result_data[-1,2], self.result_data[-1,3]))\n",
    "\n",
    "        #save final performance\n",
    "        self.save_training_data(data)            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize data: original data is overwritten\n",
    "data_min, data_max = torch.min(training_data.data), torch.max(training_data.data)\n",
    "print('original min-max values are: %r, %r and type %r' % (data_min.item(), data_max.item(), data_min.dtype))\n",
    "\n",
    "normalize_tpye = 0\n",
    "\n",
    "if normalize_tpye == 0:\n",
    "    #min-max-rescaling\n",
    "    training_data.data = (training_data.data.float() - data_min) / (data_max - data_min)    \n",
    "    test_data.data = (test_data.data.float() - data_min) / (data_max - data_min)   \n",
    "else:\n",
    "    #min-max-normalization\n",
    "    training_data.data = 2*(training_data.data.float() - data_min) / (data_max - data_min) - 1\n",
    "    test_data.data = 2*(test_data.data.float() - data_min) / (data_max - data_min) - 1 \n",
    "\n",
    "data_min, data_max = torch.min(training_data.data), torch.max(training_data.data)\n",
    "print('now min-max values are: %r, %r and type %r' % (data_min.item(), data_max.item(), data_min.dtype))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define X and Y values and do optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input is flattend to n x 784\n",
    "data_X = training_data.data.flatten(1)\n",
    "data_X_test = test_data.data.flatten(1)\n",
    "#labels are direclty supported by pytorch\n",
    "data_Y = training_data.targets\n",
    "data_Y_test = test_data.targets\n",
    "\n",
    "data = {'X_train' : data_X, 'Y_train' : data_Y, \\\n",
    "         'X_val' : data_X_test, 'Y_val' : data_Y_test}\n",
    "\n",
    "num_input = data_X.shape[1]\n",
    "num_output = len(torch.unique(data_Y))\n",
    "num_hidden = 100\n",
    "\n",
    "mlp = MultiLayerPerceptron(num_input, num_hidden, num_output)\n",
    "\n",
    "print(mlp.model)\n",
    "\n",
    "mlp.optimize(data, 20, 0.04, 1, 2)\n",
    "\n",
    "#plot the results (ranges to the right used for fig.11 - fig.13\n",
    "plot_cost(mlp, y_range = [2e-1, 2.5]) #[1e-1, 2.5]\n",
    "plot_error(mlp, y_range = [1e-1, 1])  #[.2e-1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = torch.argmax(mlp.model(data_X),1)\n",
    "\n",
    "#select a number of false classifications (rows x cols) to plot\n",
    "num_sel = 9\n",
    "\n",
    "plot_tiles(training_data.data[y_pred != data_Y], num_sel, num_sel, figure_size = [6,6])\n",
    "\n",
    "for i0 in range(0, num_sel):\n",
    "    print(y_pred[y_pred != data_Y][i0*num_sel:(i0+1)*num_sel].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input is flattend to n x 784\n",
    "data_X = test_data.data.flatten(1)\n",
    "#labels are direclty supported by pytorch\n",
    "data_Y = test_data.targets\n",
    "\n",
    "y_pred = torch.argmax(mlp.model(data_X),1)\n",
    "\n",
    "test_acc = torch.sum(data_Y != y_pred)/data_Y.shape[0]\n",
    "print(test_acc.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.sum(data_Y != y_pred), data_Y.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
