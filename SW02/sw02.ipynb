{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "![hslu_logo.png](./img/hslu_logo.png)\n",
    "\n",
    "\n",
    "<hr style=\"border:1px solid black\">\n",
    "\n",
    "<h1 style=\"text-align:center;font-size:50px\"><b>AAI - FS25</b></h1>\n",
    "<p style=\"text-align:center;font-size:40px\">Week 02</p>\n",
    "\n",
    "---\n",
    "\n",
    "# Feadforward Neural Networks (NN)\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    " \n",
    "# Table of contents for week 02\n",
    "1. [Multi Layer Perceptron](#multi_perceptron)\n",
    "2. [Data Preparation](#data_prep)\n",
    "   - [Split of data in Training and Test Set](#data_split)\n",
    "   - [Data Normalisation – Scaling and Centring](#data_norm)\n",
    "3. [Extension of Gradient Descent – Stochastic and Mini Batch GD](#extension)\n",
    "   -  [Stochastic Gradient Descent](#SGD)\n",
    "   -  [Mini-Batch Gradient Descent](#MBGD)\n",
    "4. [Model Selection Process](#model_selection)\n",
    "   - [k-fold Cross-Validation](#k_fold_x_valid)\n",
    "4. [Annex](#annex)\n",
    "   - [Backpropagation](#backprop)\n",
    "\n",
    "---\n",
    "\n",
    "# Multi Layer Perceptron (MLP)<a name=\"multi_perceptron\"></a>\n",
    "Based on the work done in the previous chapters we can now generalize our consideration on multi-layer architectures. A deep *feedforward* network, also called feedforward neural network, or multi-layer Perceptron (MLP), which is simply a stack of single layer LTUs, is the quintessential deep learning model. These models are called feedforward because information ﬂows through the function being evaluated from the input vector $\\mathbf{x}$ through the intermediate computations used to deﬁne the output $\\hat{y}=h_θ (\\mathbf{x})$. There are no feedback connections in which outputs of the model are fed back into itself. When feedforward neural networks are extended to include feedback connections, they are called recurrent neural which are discussed in the second part of the lecture. \n",
    "In <a href=\"#fig1\">Fig.1</a> the simplest MLP-architecture is shown with just one additional layer – between the in- and output layer. These intermediate layers are called *hidden* layers and are characterized by their respective number of neurons (here $n_1$). The represented hidden layer is of fully connected type and each input neuron is connected to each hidden neuron (<span style=\"color:green\">O</span><-><span style=\"color:blue\">O</span>). In addition, each hidden neuron is connected to each neuron of the softmax output layer (<span style=\"color:blue\">O</span><-><span style=\"color:blue\">O</span>). Finally, each neuron (hidden and softmax) is connected to a bias neuron (<span style=\"color:yellow\">O</span><-><span style=\"color:blue\">O</span>). \n",
    "\n",
    "<img src=\"img/mlp.png\" alt=\"Drawing\" width=\"550\" />\n",
    "<a id=\"fig1\">Fig 1:</a> A Multi-Layer Perceptron with one hidden layer. \n",
    "\n",
    "---\n",
    "\n",
    "It would be obvious now to add further hidden layers but the question arises whether this would bring any advantage. If we recall the goal of a machine learning task (c.f. Fig.1 in [sw01.ipynb](http://localhost:8888/notebooks/Kursunterlagen/SW01/sw01.ipynb#)) we want our model function $h_θ (\\mathbf{x})$ to approximate the unknow mapping $f(x)$ through the minimisation of a given loss function. However, before we start to add hidden layers to our MLP we should be aware of the *reprentational capacity* of a MLP with given number of hidden layers i.e., the class of functions that can be approximated with the given architecture. \n",
    "<br>\n",
    "In <a href=\"#fig2\">Fig.2</a> the representational capacity of the MLP with the indicated number of hidden layers - and one single output neuron thus treating a binary decsion problem - is illustrated. As already discussed above the decision boundary of a Perceptron (a) - no hidden layers - represents a hyperplane i.e., a line in 2D-space. With one single hidden layer (b) we combine hyperplanes and the decision boundary can surround any convex region. However, it should be noted that more complex cases are possible with one single hidden layer. Finally, it turns out that an MLP with two hidden layers (c) can produce any arbitrary decision boundary. \n",
    "\n",
    "<img src=\"img/mlp_capacity.png\" alt=\"Drawing\" width=\"450\" />\n",
    "<a id=\"fig2\">Fig.2:</a> The number of hidden  layers – (a)= 0, (b) = 1, (c) = 2 – determines the capacity of the model to form decision boundaries. \n",
    "\n",
    "---\n",
    "\n",
    "We will now have a look at these three cases in detail.\n",
    "\n",
    "## Decision Boundary for Zero hidden Layer – Perceptron\n",
    "We already know from the discussion on the Perceptron above that its decision boundary is a simple hyperplane in the respective feature space. This can be considered as the case with zero hidden layers. We recall this case because it is the basis for the understanding of the two following, more complex cases.\n",
    "\n",
    "<img src=\"img/no_hidden.png\" alt=\"Drawing\" width=\"550\" />\n",
    "<a id=\"fig3\">Fig.3:</a> For the Perceptron the decision boundary is a hyperplane. \n",
    "\n",
    "---\n",
    "\n",
    "## Decision Boundary for One hidden Layer\n",
    "Now we consider the case of one hidden layer as represented in <a href=\"#fig4\">Fig.4</a> below. Each neuron of the hidden layer will define one decision boundary, represented by its respective colour. If the bias of the output neuron is set to -3 the output $h_θ (\\mathbf{x})$ will only be equal or larger than zero if all three hidden neurons are equal to one. This corresponds to the triangle defined by the three decision boundaries, which is the intersection of the three corresponding decision regions.\n",
    "\n",
    "<img src=\"img/single_hidden.png\" alt=\"Drawing\" width=\"550\" />\n",
    "<a id=\"fig4\">Fig.4:</a> For one hidden layer decision boundaries surrounding any arbitrary convex region can be produced. \n",
    "\n",
    "---\n",
    "\n",
    "## Decision Boundary for Two hidden Layers\n",
    "Finally, we consider the case of two hidden layers as represented in <a href=\"#fig5\">Fig.5</a> below. We combine the neurons of the first hidden layer in groups, represented by the different colours. Each group – the vertical dots indicate additional neurons being part of that group – will, together with the corresponding neuron of the second hidden layer, define a decision boundary for a convex region. Just as explained in the previous chapter. The horizontal dots indicate that there might be additional groups representing addition convex regions. Now, if the bias of the output neuron is set to $-1$, $h_θ (\\mathbf{x})$ will be equal or larger than zero if at least one region contributes. Thus, the overall decision region corresponds to the union of all convex regions and we can therefore form any arbitrary region. \n",
    "\n",
    "<img src=\"img/two_hidden.png\" alt=\"Drawing\" width=\"550\" />\n",
    "<a id=\"fig5\">Fig.5:</a> For two hidden layers decision boundaries surrounding any arbitrary region can be produced . \n",
    "\n",
    "---\n",
    "\n",
    "Thus we see that in principle it is of no use to add more than two *dense* hidden layers to a MLP because it will not increase the representational capacity. But we will see later when treating Convolutional Neural Networks that for other layer types *depth* is crucial for high accuracy. Furthermore by increasing the number of output neurons more complex than binary classification problems can be handled with the respective number of hidden layers.\n",
    "\n",
    "---\n",
    "\n",
    "**Exercise:** \n",
    "**[sw02.01.multi_layer_perceptron.ipynb](http://localhost:8888/notebooks/Kursunterlagen/SW02/sw02.01.multi_layer_perceptron.ipynb#)**\n",
    "\n",
    "We now want to extend the notebook [sw01.03.single_layer_perceptron.ipynb](http://localhost:8888/notebooks/Kursunterlagen/SW01/sw01.03.single_layer_perceptron.ipynb#) with additional hidden layers to study the representational capacity of the MLP. We limit our problem to a binary decision.\n",
    "\n",
    "- Cell [2] `Test the creation of data`:\n",
    "  Change the image file `Regions00.png` to create a problem, that cannot be solved without a hidden layer. E.g. the following version would do the job (make yourself clear why this is the case):\n",
    "  \n",
    "<img src=\"img/Regions01.png\" alt=\"Drawing\" width=\"350\" />\n",
    "\n",
    "- Cell [3] `Define the Neural Network`:<br>\n",
    "  Now further extend the implementation to add a hidden layer with a configurable number of neurons. Therefore you have to add a `torch.nn.Linear`\n",
    "  layer with `num_input` of `in_features`(1st parameter) and the chosen number of hidden neurons as `out_features` (2nd parameter). In addition, you\n",
    "  have to choose an activation function for the hidden layer. You may choose the sigmoid function `torch.nn.Sigmoid`, or may try the rectified linear\n",
    "  unit `torch.nn.ReLU`.\n",
    "\n",
    "- Cell [6] `Setup Perceptron and do optimization`:<br>\n",
    "  Here the number of hidden neurons can be chosen (`num_hidden = 3`). Start the training and observe the result of the statement `print(mlp.model)`, which prints the model's architecture and provides a possibilty to verify the implementation. Try to reduce the error rate to zero - by adjusting the number of hidden neurons - and also observe the result graphically using cell [9]. A possible outcome could look like below:\n",
    "\n",
    "<img src=\"img/class_result_2.png\" alt=\"Drawing\" width=\"550\" />\n",
    "\n",
    "- You may want to formulate through a new image a further - more complex - problem which requires a second hidden layer. An example is given below.\n",
    "  However, it may be hard to make the learning converge without an extension to Gradient Descend - so-called *Stochastic GD* - which we will discuss later.\n",
    "\n",
    "<img src=\"img/Regions03.png\" alt=\"Drawing\" width=\"350\" />\n",
    "\n",
    "---\n",
    "\n",
    "We have seen in SW01 that the gradient calculation for application of the Gradient Descent algorithms is done automatically by PyTorch (or any other framework you may want to use). While in principle we could simply rely on PyTorch, we should nevertheless develop a basic understanding on how the gradient calculation for complex NN architectures is done. In the <a href=\"#annex\">annex</a> this is explained using a simple example.\n",
    "\n",
    "---\n",
    "\n",
    "**Exercise:** \n",
    "**[sw02.02.mlp_fashion-mnist.ipynb](http://localhost:8888/notebooks/Kursunterlagen/SW02/sw02.02.mlp_fashion-mnist.ipynb#)**\n",
    "\n",
    "Now we also want to extend or \"toy model\" [sw01.04.perceptron_fashion-mnist.ipynb](http://localhost:8888/notebooks/Kursunterlagen/SW01/sw01.04.perceptron_fashion-mnist.ipynb#) and introduce one or several hidden layers. \n",
    "\n",
    "As for the previous exercise complete the cell `Define the neural network` with the implementation of the missing hidden layer. Look out for the remark:\n",
    "\n",
    "*### START YOUR CODE ###*\n",
    "\n",
    "*### END YOUR CODE ###*\n",
    "\n",
    "Then start the training i.e., the Gradient Descend algorithm and verify that the training converges.\n",
    "\n",
    "---\n",
    "\n",
    "# Data Preparation <a name=\"data_prep\"></a>\n",
    "\n",
    "## Split of data in Training and Test Set <a name=\"data_split\"></a>\n",
    "After the learning procedure we want to test our machine learning model. This must be done on an independent data set, because we want to make a prediction about the ability of our model to generalize to unknown data. Therefore, we split the data into two subsets :\n",
    "\n",
    "- Training set: Used for learning the task.\n",
    "- Test set: Used for testing how well the learned model performs.\n",
    "\n",
    "The typical split ratios used depend on the available data size. They are for small and large ($≳10^6$) datasets respectively:\n",
    "\n",
    "<br>\n",
    "<img src=\"img/train_test.png\" alt=\"Drawing\" width=\"450\" />\n",
    "<a id=\"tab1\">Tab 1:</a> Typical ratios for training and test data fraction. \n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "The following points should be considered:\n",
    "\n",
    "- Training and test set should share the same characteristics: \n",
    "Data acquisition may change over time. E.g., the first and second halves of the image set may have been acquired under different lighting conditions (day <-> night). To ensure identical characteristics the data should be shuffled randomly before splitting.\n",
    "\n",
    "- Training and test set should be kept strictly separated:\n",
    "During learning, no information contained in the test set may be used to adjust parameters of the model. As mentioned above, this is the only way to make an independent statement about the performance of the model on previously unseen data.\n",
    "\n",
    "\n",
    "In our toy model (e.g. [sw02.02.mlp_fashion-mnist.ipynb](http://localhost:8888/notebooks/Kursunterlagen/SW02/sw02.02.mlp_fashion-mnist.ipynb#)) the function `read_data` in cell [2] returns the two sets in the variables `training_data` and `test_data` respectively.\n",
    "\n",
    "## Data Normalisation – Scaling and Centring<a name=\"data_norm\"></a>\n",
    "\n",
    "Data normalization is the process of scaling and centring the data:\n",
    "\n",
    "- Scaling means to bring the data to the same scale.\n",
    "It improves the numerical stability, the convergence speed and accuracy of the learning algorithms. We will illustrate the reason for that that below.\n",
    "\n",
    "- Centring means to balance the data around zero. \n",
    "This also improves the robustness of the learning algorithms and is particularly important for stabilising the convergence in deep networks. Probably it also improves the convergence speed and accuracy of the learning algorithms.\n",
    " \n",
    "#### Illustration of Scaling of input data:\n",
    "\n",
    "<img src=\"img/feature_scaling.png\" alt=\"Drawing\" width=\"600\" />\n",
    "<a id=\"fig6\">Fig.6:</a> Feature scaling will lead to weights having also similar scales which will be beneficial for the stability of the learning process.\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "The numerical values of the features contained in the input  data $x$ may be on different scales, ranging over different orders of magnitude. E.g., the input data for a classification problem on large cities may contain the following features:\n",
    "\n",
    "1. Size in km measuring the perimeter (~ 100)\n",
    "2. Population (~ 1’000’000)\n",
    "3. Yearly gross product in € (~ 10’000’000’000) \n",
    "\n",
    "If we leave the input data on such different scales the corresponding weights in the Perceptron will also have very different scales. The reason for that is illustrated in <a href=\"#fig6\">Fig.6</a>. The activation function $H(z)$  has a dimensionless argument $z$ and therefore, the weight component $w_k$ will have the inverse unit and inverse scale of the corresponding input feature $x_k$ such that the value of the product will be of unit magnitude i.e., $w \\cdot x=\\sum{w_k\\cdot x_k}≈1$. Thus, if we scale the input features to have the same magnitude this will be also true for the weights (<a href=\"#fig6\">Fig.6</a>). This is the preferred case as the learning algorithms can focus on learning the importance of features and not their scale.\n",
    "\n",
    "Different scaling and normalization schemes are applied and will be presented below.\n",
    "\n",
    "### Min-Max Rescaling\n",
    "The input values $\\mathbf{x}$ consist of a set of training vectors $\\mathbf{x}^{(i)}$ with index $i=1..m$. Each vector consists of components $x_k^{(i)}$ with index $k=1..n$. For the example given above with the large city classification each vector $\\mathbf{x}^{(i)}$ (each city) consists of three components ($n=3$) corresponding to the size, population, and gross product. To prepare the scaling, for each component $k$ the minimum and maximum ($\\min_k, \\max_k$) over all samples are determined: \n",
    "\n",
    "<img src=\"img/min_max.png\" alt=\"Drawing\" width=\"300\" />\n",
    "\n",
    "Thus, for all cities the maximum and minimum size, population, and gross product are determined. It should be noted that the minimum and maximum is taken over the *training data* set only to keep the strict separation of the learning process from the test set. \n",
    "Then the following scaling scheme is applied, which will map all values, independent from their input scales, to the final range $[0,1]$:\n",
    "\n",
    "<img src=\"img/min_max_rescaling.png\" alt=\"Drawing\" width=\"240\" />\n",
    "\n",
    "It is important to note that for images the minimum and maximum values are determined over all features i.e., pixels of the images. Recall e.g., that for the MNIST dataset each vector $\\mathbf{x}^{(i)}$ represents an image with $n=28×28=784$ components corresponding to the pixels of the image patches. Because all pixels of the image are already at the same scale and their respective scale is important - e.g. background pixel may be darker as foreground - there is no need to scale them independently. This is illustrated in the <a href=\"#fig7\">Fig.7</a>, where the scaling is applied to one single MNIST light image. Note that the value of $\\min_k=0$ is skipped in the formula and only the division by $\\max_k$ is applied.\n",
    "\n",
    "<img src=\"img/min_max_rescaling_mnist.png\" alt=\"Drawing\" width=\"480\" />\n",
    "<a id=\"fig7\">Fig.7:</a> Min-Max Rescaling applied to (a single) MNIST light  image.\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "Apart from Min-Max Rescaling where no centring is applied the following two normalization schemes (i.e., including centring) are applied.\n",
    "\n",
    "\n",
    "### Min-Max Normalisation\n",
    "\n",
    "Min-Max Normalisation is very similar to Min-Max Rescaling apart from the fact that now centring is applied via the following scheme mapping all values, independent from their input scales to the final range $[-1,1]$:\n",
    "\n",
    "<img src=\"img/min_max_norm.png\" alt=\"Drawing\" width=\"240\" />\n",
    "\n",
    "### z-Normalisation\n",
    "This normalisation scheme is inspired by statistics as it leads to a distribution of each input component having zero mean and unit-variance. Therefore, in a first step the mean and variance for each input component is determined (on the *training* data only):\n",
    "\n",
    "$$\n",
    "\\mu_k=\\frac{1}{m}\\sum_{i=1}^{m} x_k^{(i)}   \\qquad \\sigma_k^2=\\frac{1}{m}\\sum_{i=1}^{m} \\left(x_k^{(i)} - \\mu_k \\right)^2\n",
    "$$\n",
    "\n",
    "Then the normalization scheme is applied, which leads to zero mean and unit variance for each input component:\n",
    "\n",
    "<img src=\"img/z-norm.png\" alt=\"Drawing\" width=\"150\" />\n",
    "\n",
    "As already discussed for Min-Max Rescaling, in case that the input data are images the mean $μ_k$ and variance $σ_k^2$ is calculated over *all* pixel i.e., all input features $k$.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**Exercise:** \n",
    "**[sw02.03.mlp_fashion-mnist_ext.ipynb](http://localhost:8888/notebooks/Kursunterlagen/SW02/sw02.03.mlp_fashion-mnist_ext.ipynb#)**\n",
    "\n",
    "We want to further extend or toy model and introduce the possibility to monitor the learning curves i.e. the behavior of the error and cost during the learning procedure. \n",
    "\n",
    "As for the previous exercise complete the cell `Define the neural network` with the implementation of the missing hidden layer. Look out for the remark:\n",
    "\n",
    "*### START YOUR CODE ###*\n",
    "\n",
    "*### END YOUR CODE ###*\n",
    "\n",
    "Furthermore have a look at the following cells:\n",
    "\n",
    "- Cell [7] `Define the neural network`:<br>\n",
    "  We have added the method `save_training_data`, which will store the cost and error values both for the training and test data. For a reason which we will explain below, we call the test data `validation` data. The method `save_training_data` is called in `optimize` at the beginning of each epoch. Thus, to store also the final results the method is called at the very end of the training procedure.\n",
    "\n",
    "- Cell [8] `normalize data`:<br>\n",
    "  We have been using this cell since the beginning but now we understand the role of the scaling (you may also choose normalization).\n",
    "\n",
    "- Cell [9] `Define X and Y values and do optimization`:<br>\n",
    "  Here we hand over both the training and the test data to the `otpimize` method of the multi-layer perceptron and after the training the functions `plot_cost` and `plot_error` are called to show the results (both defined in the file `utils.py`).\n",
    "\n",
    "- Perform several training runs with different learning rate starting with $\\alpha=0.5$ and then reducing its value further. Reproduce the results of the following <a href=\"#fig8\">Fig.8</a> (all with `num_hidden = 100`). Observe and explain the results.\n",
    "\n",
    "\n",
    "<img src=\"img/BGD_cost_error.png\" alt=\"Drawing\" width=\"1000\" />\n",
    "<a id=\"fig8\">Fig.8:</a> Cost and error as function of training epoch for different learning rates $\\alpha$.\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Extension of Gradient Descent – Stochastic and Mini Batch GD <a name=\"extension\"></a>\n",
    "\n",
    "So far, we have computed the gradient of the cost function defined on the entire training set. This procedure is referred to as Batch Gradient Descent. This computation can be costly because it involves the evaluation of the model and its derivatives for all the samples in the training set. This is not an issue for small models like the single (layer) perceptron but for deep neural networks and if the training dataset is large this becomes a problem. <br>\n",
    "The idea of so-called Stochastic Gradient Descent and Mini-Batch Gradient Descent is based on the following observation. Recall the formulation  for the general CE cost (MSE cost similar). It is expressed as an arithmetic mean over the per sample contributions (\"Loss\"): \n",
    "<img src=\"img/CE_cost_SGD.png\" alt=\"Drawing\" width=\"600\" />\n",
    "\n",
    "*Remark:*<br> \n",
    "When we refer to the entire sum, we use the term CE \"Cost\", when only referring to a single term, we use the term CE \"Loss\".\n",
    "\n",
    "We may assume that the update direction $∇_θ  J_{CE} (θ)$ is already approximated with fewer (than all) samples. Thus, depending on the extend of the sum the following GD variants can be formulated:\n",
    "\n",
    "- Batch GD: Averaging over all training samples.\n",
    "- Mini-Batch GD: Averaging only over a subset of training samples.\n",
    "- Stochastic GD: No averaging, just use a single randomly selected sample.\n",
    "\n",
    "It is important to note that if we do not apply Batch GD there is in general no guarantee to move in the parameter space towards a direction that leads to smaller values of the cost function. This will become especially evident for Stochastic GD.\n",
    "\n",
    "## Stochastic Gradient Descent <a name=\"SGD\"></a>\n",
    "For Stochastic GD we only select *one single* sample for each update step. We refer to an epoch as a full iteration over all samples m i.e., doing m independent GD update step with one single sample each. The formal update scheme for the binary classification with CE cost is as follows:\n",
    "\n",
    "<img src=\"img/SGD_scheme.png\" alt=\"Drawing\" width=\"750\" />\n",
    "\n",
    "As we will see the convergence may be trickier to observe as we may select favourable and less favourable points in the training set. In practice we may compute a sliding averaged cost over the past updates.\n",
    "\n",
    "\n",
    "### Results for Classification on FashionMNIST Dataset\n",
    "<a href=\"#fig9\">Fig.9</a> below shows results obtained for classification of the FashionMNIST data using a MLP (one hidden layer with 100 neurons) with CE cost function. 20 epochs with 60'000 (size of training set) up-date steps of one sample each have be performed.\n",
    "\n",
    "<img src=\"img/SGD_MNIST.png\" alt=\"Drawing\" width=\"700\" />\n",
    "<a id=\"fig9\">Fig.9:</a> Results of SGD for CE cost and error rate for classification of the FashionMNIST data.\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "One immediately observes that the cost function and error rate are not monotonically decreasing but strongly fluctuate both for training and test set. We observe a fast learning for the first few epochs at the beginning of the optimization. When comparing with Batch GD we observe that we require considerably less epochs for a “convergence” i.e., a decrease to a test error rate close to the optimum. But it should be noted that one epoch is more costly for SGD than for BGD .<br>\n",
    "When performing several runs even with identical parameter settings <a href=\"#fig10\">Fig.10</a> the results will differ due to the random selection of the samples for each GD update step. Thus, the results have intrinsically a stochastic nature and must be interpreted accordingly.\n",
    "\n",
    "<img src=\"img/SGD_MNIST1.png\" alt=\"Drawing\" width=\"700\" />\n",
    "<a id=\"fig10\">Fig.10:</a> Comparison of error rates for different runs using SGD (details see text).\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "Finally, <a href=\"#fig11\">Fig.11</a> shows the results for the same model as above now successively reducing the learning rate from left to right. While a learning rate of $\\alpha=0.04$ was suitable for BGD (c.f. <a href=\"#fig8\">Fig.8</a>) we observe that by choosing smaller learning rates we obtain much smoother behaviour of both cost function and error rates for SGD.\n",
    "\n",
    "<img src=\"img/SGD_MNIST2.png\" alt=\"Drawing\" width=\"900\" />\n",
    "<a id=\"fig11\">Fig.11:</a> Reducing the learning rate for SGD will reduce the fluctuations of both cost function and error rates.\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Summarizing the characteristics of SGD, we can state:\n",
    "\n",
    "General Characteristics<br>\n",
    "- SGD tends to move in direction of a local minimum, but not always.\n",
    "- It never converges like batch gradient descent does but ends up fluctuating around the local minimum. If we are close enough to the minimum this is not a problem.\n",
    "- It allows for escaping local minima, thus has a regularizing effect. We will address this important issue later.\n",
    "\n",
    "Advantages<br>\n",
    "- SGD needs less epochs since parameters are updated for each training sample.\n",
    "- It can handle very large sets of data and is great for learning on huge datasets that do not fit in memory (“out-of-core learning”).\n",
    "- It allows for incremental learning (“online learning”) i.e., on-the-fly adjustment of the model parameters on new incoming data.\n",
    "\n",
    "Disadvantage<br>\n",
    "- SGD cannot easily by parallelised.\n",
    "\n",
    "\n",
    "## Mini-Batch Gradient Descent <a name=\"MBGD\"></a>\n",
    "\n",
    "Mini-Batch Gradient Descent is in fact a compromise between Batch Gradient Descent and Stochastic Gradient Descent. The process is presented schematically in <a href=\"#fig12\">Fig.12</a>. For each epoch, the m training samples are randomly shuffled and m/b equally sized batches (of sizes b each) are created. Then for each update step a batch is chosen randomly and a Batch GD step is performed by evaluating the sums only over the samples contained in the batch. One epoch consists of one loop over all batches.\n",
    "\n",
    "<img src=\"img/MBGD.png\" alt=\"Drawing\" width=\"600\" />\n",
    "<a id=\"fig12\">Fig.12:</a> Schematic presentation of MBGD (details see text).\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "The formal update scheme for the classification with CE cost is as follows:\n",
    "\n",
    "<img src=\"img/MBGD_scheme.png\" alt=\"Drawing\" width=\"750\" />\n",
    "\n",
    "The <a href=\"#fig13\">Fig.13</a> below shows the results for the classification on the FashionMNIST dataset for MBGD. A batch size of $b=16$ was chosen, the learning rate was $α=0.01$. We observe that for identical learning rate the learning curves are smoother than with SGD, however, the smoothness will obviously depend on the batch size. After 50 epochs an almost optimal validation error rate of ~11.9% was reached (1193 out of 10'000 wrong) i.e., we observe – like SGD – a much faster convergence than for BGD.\n",
    "\n",
    "<img src=\"img/MBGD_MNIST.png\" alt=\"Drawing\" width=\"700\" />\n",
    "<a id=\"fig13\">Fig.13:</a> Results of CE cost and Error rate for classification of the FashionMNIST data.\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Summarizing the characteristics of MBGD, we can state:\n",
    "\n",
    "General Characteristics<br>\n",
    "- MBGD tends to move in direction of a local minimum, but not always.\n",
    "- It ends up wandering closely around the local minimum.\n",
    "- It allows for escaping local minima, thus has a regularizing effect. We will address this important issue later.\n",
    "\n",
    "Advantages<br>\n",
    "- MBGD is faster than BGD since parameters are updated for each mini-batch.\n",
    "- It can handle very large sets of data and is great for learning on huge datasets that do not fit in memory (“out-of-core learning”).\n",
    "- It allows for incremental learning (‘online learning’) i.e., on-the-fly adjustment of the model parameters on new incoming data.\n",
    "- It can be parallelised on GPU and in HPC.\n",
    "\n",
    "Disadvantage<br>\n",
    "- The only inconvenience is that we have an additional hyperparameter, the batch size, that needs to be optimised.\n",
    "\n",
    "<a href=\"#tab2\">Tab.2</a> below summarizes the properties of the three optimisation schemes. Note the difference in learning rate for the three graphs. The comparison shows that the three schemes allow for different tradeoffs concerning the learning rate, the noise or fluctuation of the learning curves, the batch size, and the possibility of parallelisation. \n",
    "\n",
    "<img src=\"img/SGD_MBGD_BGD.png\" alt=\"Drawing\" width=\"1000\" />\n",
    "<a id=\"tab2\">Tab.2</a> Summary of the differences between BGD, SGD, and MBGD. Note the difference in learning rate.\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "**Exercise:** \n",
    "**[sw02.04.mlp_fashion-mnist_SGD.ipynb](http://localhost:8888/notebooks/Kursunterlagen/SW02/sw02.04.mlp_fashion-mnist_SGD.ipynb#)**\n",
    "\n",
    "This is a further extension of our toy model now with the possibility to perform MBGD and SGD. \n",
    "\n",
    "Start working yourself through the code:\n",
    "\n",
    "- Cell [7] `MiniBatch class`:<br>\n",
    "  This is the main novelty. Here you can give a dataset (`X,Y` in the constructor) and a batchsize (`batch_size`). With each call to the method `next()` you will receive a batch (a dictionary) of batchsize samples, which you can use during the training process. <br>\n",
    "  Make sure you understand the way the class works.<br>\n",
    "  Note that PyTorch has an own dataloader that could do the same task as the class `MiniBatch`. However, the dataloader would be considerably slower here and the implementation overhead is small.\n",
    "\n",
    "- Cell [8] `Define the neural network`:<br>\n",
    "  As for the previous exercises in the constructor complete the cell `Define the neural network` with the implementation of   the missing hidden layer. Look out for the remark:\n",
    "\n",
    "  *### START YOUR CODE ###*\n",
    "\n",
    "  *### END YOUR CODE ###*\n",
    "\n",
    "- Cell [8] `Define the neural network`:<br>\n",
    "  The main point is to observe the changes in the method `optimize`.\n",
    "  Note that for each training epoch a full run over the training set is done:\n",
    "    1. Within the loop over the epochs the class <br>\n",
    "    `batches = MiniBatches(data['X_train'], data['Y_train'], batch_size)` <br>\n",
    "     is created.\n",
    "    2. An inner loop (within the epochs) over the number of batches (`batches.number_of_batches()`) is done.\n",
    "    3. The method `batch = batches.next()` is called and used the return values `batch['X_batch']` and `batch['Y_batch']` as input to the methods `propagate(...)` and `calc_cost(...)`.\n",
    "    4. The parameter `batch_size` can be set in the call to the method `optimize(...)`\n",
    "\n",
    "Once you understand the implementation continue in the script.\n",
    "\n",
    "- Cell [10] `Define X and Y values and do optimization`:<br>\n",
    "  Now trigger the training by executing this cell. The default settings (MLP with one hidden layer, 100 neurons, SGD, $\\alpha=0.04$) correspond to the settings in <a href=\"#fig9\">Fig.9</a> and <a href=\"#fig10\">Fig.10</a> above.\n",
    "  1. Reproduce qualitatively these results and then also change the learning rate as shown in <a href=\"#fig11\">Fig.11</a>.\n",
    "  2. Then change to MBGD and reproduce qualitatively the results of <a href=\"#fig13\">Fig.13</a> and <a href=\"#tab2\">Tab.2</a>.\n",
    "\n",
    "\n",
    "\n",
    "# Model Selection Process<a name=\"model_selection\"></a>\n",
    "\n",
    "From the discussion in the previous paragraph we learned that the optimisation of the learning procedure requires to tune several parameters (learning rate, batch size, number of hidden neurons, ...) carfully. Therefore we studied the model performance on training and test data set simultaneously. As discussed in the above chapter on the data split ([Ref](#data_split)) the test data set is an independent prediction of the so-called generalisation error i.e., the performance of our model on unseen data. However, to keep this prediction really independent we are not allowed to use any information from the test set during the training. To avoid this problem, we will further split the data into now three sets, as shown in the following <a href=\"#tab3\">Tab.3</a>. But before that we introduce the concept of a so-called hyperparameter:\n",
    "\n",
    "<img src=\"img/hyperparam.png\" alt=\"Drawing\" width=\"680\" />\n",
    "\n",
    "Examples of hyperparameters are:\n",
    "\n",
    "- Learning rate $α$\n",
    "- Number of hidden layers in a neural network\n",
    "- Number of neurons in a layer of a neural network\n",
    "- Degree $d$ of the polynomial of our regression problem ([sw01.02.optimization_autograd.ipynb](http://localhost:8888/notebooks/Kursunterlagen/SW01/sw01.02.optimization_autograd.ipynb#))\n",
    "- Activation function\n",
    "- Batch size\n",
    "- ...etc. (more to come)\n",
    "\n",
    "\n",
    "We can now extend the data sets by including the validation set, which specifically allows to tune the hyperparameters of the ML model.\n",
    "\n",
    "<img src=\"img/validation.png\" alt=\"Drawing\" width=\"700\" />\n",
    "<a id=\"tab3\">Tab 3:</a> The available data set is split in three parts, training, validation, and test set.\n",
    "<br>\n",
    "\n",
    "---\n",
    "The typical split ratios given to the right are for small and large ($≳10^6$) datasets (3<sup>rd</sup> and 4<sup>th</sup> column).  \n",
    "\n",
    "<sup>1)</sup>With trustable we mean:\n",
    "- composed of data acquired under the same conditions,\n",
    "- including the same characteristics (range of values, distribution, etc),\n",
    "- sufficiently large to have confidence in the parameter estimates or evaluation metrics.\n",
    "\n",
    "\n",
    "Based on these three data sets we can now formulate the standard process for Model Selection in its general form <a href=\"#fig14\">Fig.14</a> which works as follows:\n",
    "\n",
    "- We split the data according to <a href=\"#tab3\">Tab.3</a> into training, validation, and test data. The test data set is hidden somewhere and not used till the final evaluation.\n",
    "- The appropriate data normalization scheme (c.f. chapter [above](#data_norm)) is applied.\n",
    "- We chose our model $h_θ$ that depends on a set of model parameters $θ$ and additional hyperparameters.\n",
    "- The model parameters $θ$ are optimized using the training data set and an appropriate cost function (MSE or CE).\n",
    "- The obtained model is evaluated using the validation set (e.g., validation cost is studied similar to  <a href=\"#tab2\">Tab.2</a>). \n",
    "- If model improvements seem possible the hyperparameters (e.g., learning rate $α$) are adjusted and a new training iteration is performed. \n",
    "- Once the hyperparameter tuning finalized and no further improvement observed, the final performance of the model on the test set is evaluated as estimate for the generalization error. \n",
    "\n",
    "<img src=\"img/model_selection.png\" alt=\"Drawing\" width=\"460\" />\n",
    "<a id=\"fig14\">Fig 14:</a> Standard process for Model Selection process with an iterative improvement of the model using the validation set and final estimation of generalization error based on the test set.\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "A further refinement of the model selection procedure that makes efficient use of the data for a more robust estimation of the validation set performance is the so-called k-fold Cross-Validation which we are going to present now.\n",
    "\n",
    "## k-fold Cross-Validation<a name=\"k_fold_x_valid\"></a>\n",
    "\n",
    "The idea of cross validation is to determine the performance of the trained model not on a single but on $k$ different validation sets. Therefore, the training set is split in $k$ so-called folds. In <a href=\"#fig15\">Fig.15</a> this is illustrated for $k=5$. Then for each fold the following procedure is applied:\n",
    "\n",
    "- Use the selected fold as validation set.\n",
    "- Train the model on the remaining $k-1$ folds.\n",
    "- Determine the performance of the model on the selected validation fold.\n",
    "\n",
    "As overall model performance the average over the $k$ folds are reported. In addition, the variance of the performance can be determined giving an idea on how confident the values are. The model leading to the best performance, among the $k$ validation folds possible, is retained. \n",
    "\n",
    "<img src=\"img/k-fold.png\" alt=\"Drawing\" width=\"560\" />\n",
    "<a id=\"fig15\">Fig 15:</a> The idea of cross validation using $k=5$ folds.\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "The following <a href=\"#fig16\">Fig.16</a> shows a simple example of hyperparameter tuning using the Generalized Perceptron on the FashionMNIST data. The x-axis corresponds to the number of epochs chosen. Cost function was MSE and the learning rate $α=0.05$. The y-axis shows the mean validation error – over the k-folds using $k=5$ – including the error bars. The bars are an estimator for the generalization error.\n",
    "\n",
    "<img src=\"img/k-fold-fashion.png\" alt=\"Drawing\" width=\"560\" />\n",
    "<a id=\"fig16\">Fig 16:</a> Example of hyperparameter tuning (#epochs) using the Generalized Perceptron on the FashionMNIST data (MSE cost function , learning rate $α=0.05$).\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "**Exercise:** \n",
    "**[sw02.04.mlp_fashion-mnist_SGD.ipynb](http://localhost:8888/notebooks/Kursunterlagen/SW02/sw02.04.mlp_fashion-mnist_SGD.ipynb#)**\n",
    "\n",
    "We go back to the final version of our toy model (inlcuding SGD and BGD) and correct the implementation in cell [10] `Define X and Y values and do optimization` such that not the test data is used for the validation of the training but a given fraction (c.f. <a href=\"#tab3\">Tab.3</a>) of the training data. Furthermore implement 5-fold cross validation and reproduce a plot similar to <a href=\"#fig16\">Fig.16</a> (you may choose different hyperparameter values).\n",
    "\n",
    "# Annex <a name=\"annex\"></a>\n",
    "\n",
    "## Backpropagation <a name=\"backprop\"></a>\n",
    "\n",
    "We have seen in SW01 that the gradient calculation for application of the Gradient Descent algorithms is done automatically by PyTorch (or any other framework you may want to use). While in principle we could simply rely on PyTorch, we want nevertheless develop a basic understanding on how the gradient calculation for complex NN architectures is done. We start with a simple example, which allows to verify all calculations in obvious manner. We define the function following function,\n",
    "\n",
    "$$\n",
    "a=(w \\cdot x+b)^2\n",
    "$$\n",
    "\n",
    "and represent it in form of a so-called *computational graph*:\n",
    "\n",
    "<br>\n",
    "<img src=\"img/forward.png\" alt=\"Drawing\" width=\"550\" />\n",
    "<a id=\"fig17\">Fig.17:</a> Forward path for the function $a=(w \\cdot x+b)^2$.\n",
    "\n",
    "---\n",
    "\n",
    "This is a very intersting concept, because it allows to formalise the forward pass, i.e. the calculation of the prediciton $\\hat{y}=h_{\\theta}(\\mathbf{x})$, as well as the gradient calculation. Looking at  <a href=\"#fig17\">Fig.17</a> it appears also quite intuitive :\n",
    "\n",
    "- Nodes represent the operations e.g. multiplication 'x', summation '+', ...\n",
    "- Edge represent the variables e.g. input values $x$, parameters $w$, or intermediate results $q$\n",
    " \n",
    "It is now possible to determine the outut $a$ (*activation*) based on the input $x$ and the parameters $w$ and $b$ in a simple and formal way by moving step by step from left to right through the graph - *forward pass* - by calculating the intermediate result of each node, as shown in the column to the left. This is fairly obvious.\n",
    "\n",
    "However, the representation as computational graph also allows to determine the derivatives of the result $a$ with respect to $b$, $w$ and $x$ in a formal way. From the basic rules of differential calculus, it follows that the successive use of the chain rule will allow to do this in a simple manner by backpropagating though the graph from right to left. This is shown in the following <a href=\"#fig18\">Fig.18:</a>\n",
    "\n",
    "<br>\n",
    "<img src=\"img/backprop.png\" alt=\"Drawing\" width=\"550\" />\n",
    "<a id=\"fig18\">Fig.18:</a> By applying successively the chain rule the derivates of result $a$ with respect to $b$, $w$ and $x$ can be determined.\n",
    "\n",
    "---\n",
    "\n",
    "Thus, we start with the node at the extreme right, which represents the square function $a=z^2$. The derivative of $a$ with respect to $z$ is simply given by:\n",
    "$$\n",
    "\\frac{\\partial a}{\\partial z} = 2\\cdot z\n",
    "$$\n",
    "\n",
    "Now we want to determine the derivative of $a$ with respect to $b$ (lower branch of the backpropagation path after the ‘+’ node). By application of the chain rule this is simply given by:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial a}{\\partial b} = \\frac{\\partial a}{\\partial z} \\cdot \\frac{\\partial z}{\\partial b} = 2z \\cdot 1\n",
    "$$\n",
    "\n",
    "Now we continue with the upper branch of the backpropagation path after the ‘+’ node to determine the derivative of $a$ with respect to $q$, which we require as an intermediate result for the derivatives with respect to $x$ and $w$. Again, by application of the chain rule this is simply given by:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial a}{\\partial q} = \\frac{\\partial a}{\\partial z} \\cdot \\frac{\\partial z}{\\partial q} = 2z \\cdot 1\n",
    "$$\n",
    "\n",
    "Now we can finish with the derivatives of $a$ with respect to $x$ and $w$, which are given respectively by:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial a}{\\partial x}=\\frac{\\partial a}{\\partial z} \\cdot \\frac{\\partial z}{\\partial q} \\cdot \\frac{\\partial q}{\\partial x}= 2z \\cdot 1 \\cdot w \n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial a}{\\partial w}=\\frac{\\partial a}{\\partial z} \\cdot \\frac{\\partial z}{\\partial q} \\cdot \\frac{\\partial q}{\\partial w}= 2z \\cdot 1 \\cdot x\n",
    "$$\n",
    "\n",
    "The following <a href=\"#fig19\">Fig.19</a> gives some numerical values both for the forward path (green) and the backpropagation. The value of 1 in the backward propagation all to left is used as starting point for the successive multiplication of the derivatives during the backpropagation.\n",
    "\n",
    "<br>\n",
    "<img src=\"img/backprop_num.png\" alt=\"Drawing\" width=\"450\" />\n",
    "<a id=\"fig19\">Fig.19:</a> Forward and backward path with numerical values .\n",
    "\n",
    "---\n",
    "\n",
    "**Exercise:** \n",
    "Verify the values of the forward and backward path and make sure that you understand the application of the chain rule during the backpropagation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
